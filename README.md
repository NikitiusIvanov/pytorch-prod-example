# Example of inference service for an pytorch model

In this project we use:

* Model - pytorch (assuming that we have trained models weights .pth file and saved architecture)
* Inference API - FastAPI
* Containerizing service - Docker
* Deploying - Google Cloud Run
  
