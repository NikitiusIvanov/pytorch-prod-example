# Example of inference service pytorch model that deployed on google-cloud-run

In this project we use:

* Model - pytorch (assuming that we have trained models weights .pth file and saved architecture)
* Inference API - FastAPI
* Containerizing service - Docker
* Deploying - Google Cloud Run
  
